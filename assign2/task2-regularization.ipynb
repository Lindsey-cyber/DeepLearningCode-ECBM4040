{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 2: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization techniques. \n",
    "\n",
    "**Batch normalization:**\n",
    "\n",
    "When a network becomes deeper, the distribution of the parameters from hidden neurons will also shift greatly. This is one of the reasons that makes it difficult to train a deep neural network.\n",
    "\n",
    "Machine learning teaches us that normalization is a good preprocessing method to deal with such a problem. Therefore, batch normalization deploys a similar idea in neural networks by re-normalizing the hidden values of each layer's outputs before transferring them to the next layer.\n",
    "\n",
    "**Dropout:**\n",
    "\n",
    "In the last assignment, you trained a shallow network and everything looked fine. However, when the network becomes wider and deeper, one of the immediate problems you will be encountering is overfitting. The network overreacts to noise or random errors of the training data while failing to detect the underlying distribution and generalize poorly on the validation/test set.\n",
    "\n",
    "Dropout is a well-known method that can mitigate such effects. The core idea behind it is quite simple: rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "**References:**\n",
    "* https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "* https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 784)\n",
      "Training labels shape:  (60000,)\n",
      "Validation data shape:  (10000, 784)\n",
      "Validation labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, different network layers may prefer different input distribution to function more effectively. However, inputs often come in varying ranges due to the fact that:\n",
    "- Data are often batched in various ways, so the statistics of every batch does not always reflect the statistics of the whole input space;\n",
    "- Due to network training, the outputs of preceding layers constantly change even on the same data.\n",
    "\n",
    "Batch normalization proposes to address this by inserting an independent layer that transforms the preceding outputs to a **learnable** distribution before sending them as input to the layers after. Specifically, our goals are:\n",
    "1. Obtain a good **estimate** of the mean and variance of the entire input space.\n",
    "2. Use this quantity to **standardize** the input distribution to $0$ mean and unit variance.\n",
    "3. **Learn** an appropriate output mean and variance that the input should be transformed into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularly, let\n",
    "\n",
    "$$X_b = (x_1, \\dots, x_N)^T \\in R^{N \\times D}, \\quad x_i \\in R^D$$\n",
    "\n",
    "be some batched input data with batch size $N$ where $b$ is the batch index (e.g. $X_1$ denotes the $1$-st batch).\n",
    "\n",
    "Batch normalization translates to two steps:\n",
    "1. Standardization:\n",
    "   $$\n",
    "   \\hat{X}_b = \\frac{X_b - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}} = \\begin{bmatrix}\n",
    "   \\frac{x_1 - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}}, \\cdots,\n",
    "   \\frac{x_N - \\hat\\mu}{\\sqrt{\\hat\\sigma^2 + \\epsilon}}\n",
    "   \\end{bmatrix}^T \\in R^{N \\times D}\n",
    "   $$\n",
    "   where $\\hat\\mu, \\hat\\sigma^2 \\in R^D$ are the estimated mean and variance of the input space (see below).\n",
    "2. Transformation:\n",
    "   $$\n",
    "   Y_b = \\hat{X}_b \\odot \\gamma + \\beta = \\begin{bmatrix}\n",
    "   \\hat{x}_1 \\odot \\gamma + \\beta, \\cdots,\n",
    "   \\hat{x}_N \\odot \\gamma + \\beta\n",
    "   \\end{bmatrix}^T \\in R^{N \\times D} \\in R^{N \\times D}\n",
    "   $$\n",
    "   where $\\beta, \\gamma \\in R^D$ are **learnable** parameters of desired output mean and vairance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Note:</strong></span> In the left equalities of the two formulas above, the notations are chosen for simplicity consideration. As is shown by the right equalities, batch normalization is applied to **every sample** in the batch $X_b$, so the computation simply propagates through the batch dimension.\n",
    "\n",
    "<span style=\"color:red\"><strong>Hint:</strong></span> You can also code in this fashion with the help of [broadcasting mechanism](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "\n",
    "Now, let's take a closer look at what happens in the training and testing stages respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stage\n",
    "\n",
    "Observing from the above equations, batchnorm is a **parametrized** layer (by $\\hat\\mu, \\hat\\sigma^2, \\beta, \\gamma$). Particularly, $\\beta, \\gamma$ are learned through network optimization (e.g. via gradient descent). One piece of the missing puzzle is the calculation of $\\hat\\mu, \\hat\\sigma^2$.\n",
    "\n",
    "In the world of model training, we often live with the following assumptions:\n",
    "- Training data represents a good and unbiased knowledge of the data space in general.\n",
    "- Testing/validation data *can* be biased and imbalanced.\n",
    "\n",
    "Therefore, in the training stage, it is usually safe to use the **sample mean** and **sample variance** of every batch when standardizing the inputs, i.e.\n",
    "\n",
    "$$\n",
    "\\hat{X}_b = \\frac{X_b - \\bar X_b}{\\sqrt{S^2(X_b) + \\epsilon}} \\quad \\text{where} \\quad\n",
    "\\begin{cases}\n",
    "&\\bar{X}_b = \\frac{1}{N} \\sum_{i=1}^N x_i \\in R^D \\\\\n",
    "&S^2(X_b) = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{X}_b)^2 \\in R^D\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And we accumulate the these values to use in the testing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is fairly costly to iterate through the entire dataset to get the quantities when we need them, let alone that they will change everytime when the parameters in the preceding layers are updated. We prefer a way of estimating their values on-the-go, and **moving average** is just the right tool for this.\n",
    "\n",
    "At every batch $b$, we update the moving averages of $\\hat \\mu$ and $\\hat \\sigma^2$ as\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat \\mu \\gets \\beta \\hat \\mu + (1 - \\beta) \\bar{X}_b \\\\\n",
    "\\hat \\sigma^2 \\gets \\beta \\hat \\sigma^2 + (1 - \\beta) S^2(X_b) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "After the training stage is over, our $\\hat \\mu$ and $\\hat \\sigma^2$ will be ready to use in the testing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stage\n",
    "\n",
    "For standardization, since the data are now ***not*** considered unbiased anymore, we cannot use their own sample mean and sample variance. Instead, we apply $\\hat \\mu$ and $\\hat \\sigma^2$ **stored from the training stage**.\n",
    "\n",
    "There is also no reason to further update their moving averages (we don't want the values calculated from the training set to be contaminated by biased test data).\n",
    "\n",
    "And all other calculations simply follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Complete the function `bn_forward` in **./utils/reg_funcs.py**\n",
    "\n",
    "If the code is running correctly, **mean of a2_bn for train will be very close to 0 and variance of a2_bn will be close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of a2:  [-5.54895299  6.93351234 -9.51414004]\n",
      "var of a2:  [ 71.27582738  38.28448262 155.67504973]\n",
      "(train) mean of a2_bn: [-2.00499339e-16 -4.44644321e-16  2.57432964e-17]\n",
      "(train) var of a2_bn: [0.99999986 0.99999974 0.99999994]\n",
      "********************************************************************************\n",
      "Is mean correct? - True\n",
      "Is variance correct? - True\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Checking/verification code. Don't change it.     #\n",
    "####################################################\n",
    "\n",
    "from utils.reg_funcs import bn_forward\n",
    "from utils.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(2022)\n",
    "N, D, H1, H2 = 200, 64, 3, 3\n",
    "eps = 1e-5\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":eps, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn:\", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn:\", np.var(a2_bn, axis=0))\n",
    "print('*'*80)\n",
    "print(\"Is mean correct? -\", np.allclose(np.mean(a2_bn, axis=0), np.zeros_like(np.mean(a2_bn, axis=0))))\n",
    "print(\"Is variance correct? -\", np.allclose(np.var(a2_bn, axis=0), np.ones_like(np.var(a2_bn, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Test \"moving average\" and \"test mode\" of the `bn_forward` function. \n",
    "\n",
    "We expect that the test mean & variance of a2 to be fairly close to the real values and the test mean & variance of a2_bn to be close to 0 and 1 respectively.\n",
    "\n",
    "<span style=\"color:red\"><strong>Note:</strong></span> Showing \"**True**\" in this section doesn't guarantee a correct implementation. To verify your code, run the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real mean of data:  [-5.37220827  6.60338545 -9.18536976]\n",
      "real var of data:  [ 69.59934038  36.79350132 154.92350758]\n",
      "moving mean of data:  [-5.20455938  6.62394822 -8.95234272]\n",
      "moving var of data:  [ 71.56319735  37.18466888 160.30635746]\n",
      "********************************************************************************\n",
      "(test) mean of a2:  [-5.6898722   6.74882481 -9.67056048]\n",
      "(test) var of a2:  [ 71.95880679  31.8456604  156.68851536]\n",
      "(test) mean of a2_bn:  [-0.05736895  0.02047854 -0.05672582]\n",
      "(test) var of a2_bn:  [1.00552811 0.85641909 0.9774317 ]\n",
      "********************************************************************************\n",
      "Is moving mean close? - True\n",
      "Is moving variance close? - True\n",
      "Is test mean close? - True\n",
      "Is test variance close? - True\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "\n",
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-8, \"decay\":0.8}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "np.random.seed(2022)\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# Compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"(test) mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"(test) var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))\n",
    "print(\"*\"*80)\n",
    "print(\"Is moving mean close? -\", np.allclose(bn_config[\"moving_mean\"], np.mean(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is moving variance close? -\", np.allclose(bn_config[\"moving_var\"], np.var(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is test mean close? -\", np.allclose(np.mean(a2_bn, axis=0), beta, atol=2e-1))\n",
    "print(\"Is test variance close? -\", np.allclose(np.var(a2_bn, axis=0), gamma, atol=2e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply take the derivatives for back propagation. Assume that the upstream gradient from the final Loss $L$ w.r.t the layer output $Y$ is $\\nabla_Y L = G \\in R^{N \\times D}$, the backward pass is\n",
    "\n",
    "$$\n",
    "\\nabla_{X} L = \\frac{\\gamma}{\\sqrt{\\hat \\sigma^2 + \\epsilon}} \\odot G \\in R^{N \\times D}, \\quad \n",
    "\\nabla_{\\gamma} L = \\mathbb{1}^T (G \\odot \\frac{X - \\hat \\mu}{\\sqrt{\\hat \\sigma^2 + \\epsilon}}) \\in R^D, \\quad\n",
    "\\nabla_{\\beta} L = \\mathbb{1}^T G \\in R^D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Read the code `bn_backward` provided in **\"./utils/reg_funcs.py\"**. Use TensorFlow functions to verify the correctness of the backward function. \n",
    "\n",
    "<span style=\"color:red\"><strong>Hint:</strong></span> Use `tf.GradientTape()`. You may find an example usage in the instructor's verification code from Assignment 1 Task1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     15\u001b[0m dbeta_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros_like(dbeta)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# TODO: Verify the backward code. You should use  #\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# the parameters in bn_config and other variables #\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     - dbeta_tf: gradient of a2_bn w.r.t beta    #\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ENDTODO #\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Make comparison\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs a2_bn correct? \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mallclose(a2_bn, a2_bn_check)))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "# You will use bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "# Results\n",
    "da2_tf = tf.zeros_like(da2)\n",
    "dgamma_tf = tf.zeros_like(dgamma)\n",
    "dbeta_tf = tf.zeros_like(dbeta)\n",
    "\n",
    "###################################################\n",
    "# TODO: Verify the backward code. You should use  #\n",
    "# the parameters in bn_config and other variables #\n",
    "# above.                                          #\n",
    "# You should store:                               #\n",
    "#     - da2_tf: gradient of a2_bn w.r.t a2        #\n",
    "#     - dgamma_tf: gradient of a2_bn w.r.t gamma  #\n",
    "#     - dbeta_tf: gradient of a2_bn w.r.t beta    #\n",
    "###################################################\n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "###################################################\n",
    "# ENDTODO #\n",
    "###################################################\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP in `./utils/neuralnets/mlp.py`\n",
    "\n",
    "2. First create a shallow MLP like two-layer network with shape [50(+10)] (a hidden layer with depth 5 and an output layer of 10 classes). Train it without and with batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "3. Then, create a slightly deeper 5-layer MLP network with shape [100,50,50,100(+10)] and train the network with and without batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment.\n",
    "\n",
    "<span style=\"color:red\"><strong>Note:</strong></span> This section serves as a proof of concept. You may play arround with the hyperparameters for a bit and build more understanding, but it is not required to achieve some certain accuracy threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP \n",
    "from utils.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamOptim(model)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# hist_no_dropout contains loss, train acc and valid acc history.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m hist_shallow_no_bn \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     11\u001b[0m     X_train, y_train, X_val, y_val, \n\u001b[1;32m     12\u001b[0m     num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, learning_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, \n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:76\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(preds, y_batch)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Update gradients after each batch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m record_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m record_interval\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:272\u001b[0m, in \u001b[0;36mAdamOptim.step\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    267\u001b[0m grads \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgrads\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO: Adam, Update mean and variance and params #\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build a two-layer network without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamOptim(model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# hist_no_dropout contains loss, train acc and valid acc history.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m hist_deep_no_bn \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     10\u001b[0m     X_train, y_train, X_val, y_val, \n\u001b[1;32m     11\u001b[0m     num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, learning_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, \n\u001b[1;32m     12\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:76\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(preds, y_batch)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Update gradients after each batch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m record_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m record_interval\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:272\u001b[0m, in \u001b[0;36mAdamOptim.step\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    267\u001b[0m grads \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgrads\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO: Adam, Update mean and variance and params #\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build a deep 5-layer network without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 100], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 100], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_deep_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_deep_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>  **[fill in here]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is another straightforward regularization technique that uses randomness to enforce the learning on all hidden neurons.\n",
    "\n",
    "### Training Stage\n",
    "\n",
    "During the forward pass of dropout, each entry in the original data is kept with a certain probability $p$, otherwise they are discarded (set to zero). Assume the input $X \\in R^{N \\times D}$, the forward pass is\n",
    "\n",
    "$$Y = M \\odot X \\in R^{N \\times D}$$\n",
    "\n",
    "where $M \\sim B(N \\times D, p)$ is a boolean mask generated from Binomial distribution.\n",
    "\n",
    "### Testing Stage\n",
    "\n",
    "In the testing stage, randomness is no longer desired, but we still need to retain the mean and variance of the output so that it conforms with the training. Therefore, we use\n",
    "\n",
    "$$Y = p X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout_forward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Complete the function `dropout_forward` in `./utils/reg_funcs.py`. If the code is running correctly, you should observe the outputs of the verification code to be close to each other. \n",
    "\n",
    "If the function is correct, then **the output mean should be close to the input mean. Input mean and output test mean should be identical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_of_input = 7.003444611253857\n",
      "mean_of_out = 6.999957797104591\n",
      "mean_of_out_test = 7.003444611253857\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "from utils.reg_funcs import dropout_forward\n",
    "from utils.reg_funcs import dropout_backward\n",
    "\n",
    "x_in = np.random.randn(500, 500) + 10\n",
    "\n",
    "p = 0.7\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into the MLP in `./utils/neuralnets/mlp.py` and understand how the dropout is added into the MLP.\n",
    "\n",
    "2. Customize your own MLP network. Then, train networks with different $p$ of $\\{0.1, 0.3, 0.5, 0.7, 0.9, 1\\}$. If $p = 1$, then the network is equivalent to the MLP without dropout. \n",
    "\n",
    "3. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "Note that checking/validation code is included below with preselected dropout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamOptim(model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# hist_no_dropout contains loss, train acc and valid acc history.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m hist_no_dropout \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     10\u001b[0m     X_train, y_train, X_val, y_val, \n\u001b[1;32m     11\u001b[0m     num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, learning_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, \n\u001b[1;32m     12\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:76\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(preds, y_batch)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Update gradients after each batch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m record_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m record_interval\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:272\u001b[0m, in \u001b[0;36mAdamOptim.step\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    267\u001b[0m grads \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgrads\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO: Adam, Update mean and variance and params #\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m###################################################\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": 1}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\": True, \"keep_prob\": 0.9}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_1 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.7}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_2 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.5}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_3 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.3}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_4 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.1}\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_5 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = hist_no_dropout\n",
    "loss_dropout_1, train_acc_dropout_1, val_acc_dropout_1 = hist_dropout_1\n",
    "loss_dropout_2, train_acc_dropout_2, val_acc_dropout_2 = hist_dropout_2\n",
    "loss_dropout_3, train_acc_dropout_3, val_acc_dropout_3 = hist_dropout_3\n",
    "loss_dropout_4, train_acc_dropout_4, val_acc_dropout_4 = hist_dropout_4\n",
    "loss_dropout_5, train_acc_dropout_5, val_acc_dropout_5 = hist_dropout_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_no_dropout, label=\"no dropout\")\n",
    "plt.plot(loss_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(loss_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(loss_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(loss_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(loss_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(train_acc_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(train_acc_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(train_acc_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(train_acc_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(train_acc_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(val_acc_dropout_1, label=\"keep_prob=0.9\")\n",
    "plt.plot(val_acc_dropout_2, label=\"keep_prob=0.7\")\n",
    "plt.plot(val_acc_dropout_3, label=\"keep_prob=0.5\")\n",
    "plt.plot(val_acc_dropout_4, label=\"keep_prob=0.3\")\n",
    "plt.plot(val_acc_dropout_5, label=\"keep_prob=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>  **[fill in here]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (128,) (64,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamOptim(model)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# hist_no_dropout contains loss, train acc and valid acc history.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m hist_dropout_5 \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     11\u001b[0m     X_train, y_train, X_val, y_val, \n\u001b[1;32m     12\u001b[0m     num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, learning_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, \n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/optimizers.py:71\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_batch \u001b[38;5;241m=\u001b[39m y_batch\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(preds, y_batch)\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/neuralnets/mlp.py:114\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m     beta \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn_beta_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)]\n\u001b[1;32m    113\u001b[0m     cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)\n\u001b[0;32m--> 114\u001b[0m     x, cache[cache_name] \u001b[38;5;241m=\u001b[39m bn_forward(x, gamma, beta, bn_params[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# END OF BATCH NORMALIZATION                  #\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ReLU\u001b[39;00m\n\u001b[1;32m    121\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)\n",
      "File \u001b[0;32m~/Documents/GitHub/e4040-2024fall-assign2-Lindsey-cyber/utils/reg_funcs.py:64\u001b[0m, in \u001b[0;36mbn_forward\u001b[0;34m(x, gamma, beta, bn_params, mode)\u001b[0m\n\u001b[1;32m     62\u001b[0m var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(x, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(var \u001b[38;5;241m+\u001b[39m eps)\n\u001b[0;32m---> 64\u001b[0m moving_mean \u001b[38;5;241m=\u001b[39m decay \u001b[38;5;241m*\u001b[39m moving_mean \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m decay) \u001b[38;5;241m*\u001b[39m mean\n\u001b[1;32m     65\u001b[0m moving_var \u001b[38;5;241m=\u001b[39m decay \u001b[38;5;241m*\u001b[39m moving_var \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m decay) \u001b[38;5;241m*\u001b[39m var\n\u001b[1;32m     66\u001b[0m bn_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoving_mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m moving_mean\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (128,) (64,) "
     ]
    }
   ],
   "source": [
    "# Create a deep network with both dropout and batch normalization.\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": 0.9}\n",
    "use_bn = True\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[128, 64, 32], num_classes=10, \n",
    "    weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn, dropout_config=dropout_config\n",
    ")\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_dropout_5 = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=5, batch_size=100, learning_rate=1e-4, learning_decay=0.95, \n",
    "    verbose=False, record_interval=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

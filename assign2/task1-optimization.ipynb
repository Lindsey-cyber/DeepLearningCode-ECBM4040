{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5a8lAWOfmmy"
   },
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_bepwPRfmnM"
   },
   "source": [
    "## Assignment 2 - Task 1: Optimization\n",
    "\n",
    "In this task, we introduce several improved optimization algorithms based on stochastic gradient descent (SGD). Naive SGD is a reasonable method to update neural network parameters. However, there exists two main drawbacks:\n",
    "\n",
    "- First, to make SGD perform well, one would need to find an appropriate learning rate and good initial values for the prameters. The training will progress slowly if the learning rate is small, or diverge if the learning rate is too large. Since we often have no prior knowledge about the training data in reality, it is not trivial to find a good learning rate by hand. Also, when the network grows deeper, one may need to set a different learning rate for each layer. \n",
    "\n",
    "- Second, SGD strictly follows the gradients of the **batched data** when updating the parameters. This can be problematic with real-world problems as has been demonstrated in the lectures.\n",
    "\n",
    "To seek for improvements of naive SGD, momentum, parameter estimation and adaptive learning rate methods are commonly the ones to rely on. Here, you are going to experiment with **SGD with Momentum**, **SGD with Nesterov-accelerated Momentum**, **Adam**, **AdamW**, **RMSProp** and compare their performances.\n",
    "\n",
    "Consult the slides and [text book](https://www.deeplearningbook.org) for details. Here is also [a useful link](http://ruder.io/optimizing-gradient-descent/) to learn more about some methods used in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5kwQ5NoOfmnP"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da7zcFQffmnd"
   },
   "source": [
    "## Load Fashion-MNIST\n",
    "\n",
    "Here we use a small dataset with only 2500 samples to simulate the \"lack-of-data\" situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1631138412954,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "PpxLY-MWfmne",
    "outputId": "dc0a48ce-34f5-41bb-be66-23a1fb1389bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (2000, 784)\n",
      "Training labels shape:  (2000,)\n",
      "Validation data shape:  (500, 784)\n",
      "Validation labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "#Consider a subset of 2500 samples of the 60000 total images (indexed 10000 ~ 12500)\n",
    "X_val = X_train[10000:10500,:]\n",
    "y_val = y_train[10000:10500]\n",
    "X_train = X_train[10500:12500,:]\n",
    "y_train = y_train[10500:12500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We have vectorized the data for you\n",
    "# Flatten the 32×32×3 images into 1×3072 Numpy arrays\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20tnMCjyfmnh"
   },
   "source": [
    "## Part 1: Implement Several Optimizers (16%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide code snippets for testing your code implementations.\n",
    "\n",
    "The best anticipated achievable accuracies are specific for each algorithm. You may use these accuracies to judge the correctness of your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "loz33g8Ufmnk"
   },
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "Assume that the goal is to optimize an objective function $L$ parametrized by network weights $\\theta \\in R^d$, the update rule of an iterative optimization algorithm in general can be formulated as\n",
    "\n",
    "$$\\theta_{t+1} \\gets \\theta_t + \\alpha_t p_t$$\n",
    "\n",
    "where $\\alpha_t > 0$ is the **step size** and $p$ is the **direction of update**. \n",
    "\n",
    "Both $\\alpha$ and $p$ can be proposed in numerous different ways which result in different optimizers with different performances.\n",
    "\n",
    "Note that in the following equations, we ***DO NOT*** take learning rate decay into consideration. This has been implemented in the base class `Optimizer.train()`. All optimizers will be derived from this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_4iVMT2fmnm"
   },
   "source": [
    "### Original SGD (For Comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time step $t$, let the gradient of a real-valued loss function $L$ w.r.t network parameter $\\theta$ be given by\n",
    "\n",
    "$$g_t = \\nabla_{\\theta_t} L(\\theta_t)$$\n",
    "\n",
    "and $\\theta_t$ denotes the values of the parameters at time $t$.\n",
    "\n",
    "As you have seen in previous examples, the loss $L$ is calculated from a mini-batch stochastically sampled from the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD (Stochastic Gradient Descent) algorithm is formulated as\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta g_t$$\n",
    "\n",
    "where $\\eta$ is the ***learning rate***. \n",
    "\n",
    "The final accuracy you should expect is arround 0.1-0.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4609,
     "status": "ok",
     "timestamp": 1631138989515,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "crxdrUTvfmnn",
    "outputId": "8c1cb517-e687-407d-bcf3-300b29383430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.116, new learning rate = 0.0095, number of evaluations 12\n",
      "epoch 2: valid acc = 0.192, new learning rate = 0.009025, number of evaluations 12\n",
      "epoch 3: valid acc = 0.194, new learning rate = 0.00857375, number of evaluations 12\n",
      "epoch 4: valid acc = 0.198, new learning rate = 0.0081450625, number of evaluations 12\n",
      "epoch 5: valid acc = 0.208, new learning rate = 0.007737809374999999, number of evaluations 12\n",
      "epoch 6: valid acc = 0.2, new learning rate = 0.007350918906249998, number of evaluations 12\n",
      "epoch 7: valid acc = 0.202, new learning rate = 0.006983372960937498, number of evaluations 12\n",
      "epoch 8: valid acc = 0.204, new learning rate = 0.006634204312890623, number of evaluations 12\n",
      "epoch 9: valid acc = 0.202, new learning rate = 0.006302494097246091, number of evaluations 12\n",
      "epoch 10: valid acc = 0.202, new learning rate = 0.005987369392383786, number of evaluations 12\n",
      "epoch 11: valid acc = 0.204, new learning rate = 0.005688000922764597, number of evaluations 12\n",
      "epoch 12: valid acc = 0.216, new learning rate = 0.005403600876626367, number of evaluations 12\n",
      "epoch 13: valid acc = 0.212, new learning rate = 0.005133420832795048, number of evaluations 12\n",
      "epoch 14: valid acc = 0.204, new learning rate = 0.0048767497911552955, number of evaluations 12\n",
      "epoch 15: valid acc = 0.204, new learning rate = 0.00463291230159753, number of evaluations 12\n"
     ]
    }
   ],
   "source": [
    "from utils.optimizers import SGDOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = SGDOptim(model)\n",
    "hist_sgd = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=15, batch_size=200, learning_rate=1e-2, learning_decay=0.95, \n",
    "    verbose=False, record_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZauLVUBlfmno"
   },
   "source": [
    "#### SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All gradient methods share the drawback of potentially being stuck in local minima. Momentum is one possible solution to circumvent this problem. By accumulating the \"velocities\" of past updates, the algorithm may be able to escape local minima and give faster convergence.\n",
    "\n",
    "The algorithm is formulated as two steps:\n",
    "1. Accumulate the **velocities**:\n",
    "   $$v_t = \\beta v_{t-1} + g_t$$\n",
    "2. Update the parameters:\n",
    "   $$\\theta_{t+1} = \\theta_t - \\eta v_t$$\n",
    "\n",
    "where $\\beta \\in [0, 1]$ is the decay factor controlling the effect of past velocities on the current update and $v_0$ is initialized to be $\\mathbb{0}$.\n",
    "\n",
    "The intuition is to modify the current update direction (naively $g_t$ in SGD) by accumulated past directions. You can think of it as rolling a heavy ball down the hill, where the inertia of the ball will always try to retain the direction of the current velocity instead of simply following the slope.\n",
    "\n",
    "The final accuracy you should expect is arround 0.5-0.7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Implement SGD + Momentum by completing `SGDmomentumOptim` in **./utils/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5250,
     "status": "ok",
     "timestamp": 1631139111767,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "oFTM2qtafmno",
    "outputId": "03fbd2a0-336b-4177-98dc-38cb6c5a3027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.152, new learning rate = 0.0095, number of evaluations 12\n",
      "epoch 2: valid acc = 0.198, new learning rate = 0.009025, number of evaluations 12\n",
      "epoch 3: valid acc = 0.204, new learning rate = 0.00857375, number of evaluations 12\n",
      "epoch 4: valid acc = 0.218, new learning rate = 0.0081450625, number of evaluations 12\n",
      "epoch 5: valid acc = 0.224, new learning rate = 0.007737809374999999, number of evaluations 12\n",
      "epoch 6: valid acc = 0.214, new learning rate = 0.007350918906249998, number of evaluations 12\n",
      "epoch 7: valid acc = 0.21, new learning rate = 0.006983372960937498, number of evaluations 12\n",
      "epoch 8: valid acc = 0.336, new learning rate = 0.006634204312890623, number of evaluations 12\n",
      "epoch 9: valid acc = 0.404, new learning rate = 0.006302494097246091, number of evaluations 12\n",
      "epoch 10: valid acc = 0.418, new learning rate = 0.005987369392383786, number of evaluations 12\n",
      "epoch 11: valid acc = 0.422, new learning rate = 0.005688000922764597, number of evaluations 12\n",
      "epoch 12: valid acc = 0.406, new learning rate = 0.005403600876626367, number of evaluations 12\n",
      "epoch 13: valid acc = 0.434, new learning rate = 0.005133420832795048, number of evaluations 12\n",
      "epoch 14: valid acc = 0.508, new learning rate = 0.0048767497911552955, number of evaluations 12\n",
      "epoch 15: valid acc = 0.53, new learning rate = 0.00463291230159753, number of evaluations 12\n"
     ]
    }
   ],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import SGDMomentumOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = SGDMomentumOptim(model, momentum=0.8)\n",
    "hist_sgd_momentum = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=15, batch_size=200, learning_rate=1e-2, \n",
    "    learning_decay=0.95, verbose=False, record_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD + Nesterov-accelerated Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While momentum shows exciting results for such a simple algorithm, the convergence rate can be further improved by introducing the Nesterov acceleration.\n",
    "\n",
    "As an intuition, the previous algorithm only modify the parameters using the momentum at the **current** time step $v_t$. Nesterov proposed to derive an estimation of the momentum at the **next** time step $\\hat v_{t+1}$ to perform the update.\n",
    "\n",
    "This translates to the following algorithm:\n",
    "1. Accumulate the velocities:\n",
    "   $$v_t = \\beta v_{t-1} + g_t$$\n",
    "2. **Nesterov acceleration**:\n",
    "   $$\\hat v_{t+1} = \\beta v_t + g_t$$\n",
    "3. Update the parameters:\n",
    "   $$\\theta_{t+1} = \\theta_t - \\eta \\hat v_{t+1}$$\n",
    "\n",
    "The final accuracy you should expect is arround 0.7-0.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Implement SGD + Nesterov-accelerated Momentum by completing `SGDNestMomentumOptim` in **./utils/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.2, new learning rate = 0.0095, number of evaluations 12\n",
      "epoch 2: valid acc = 0.228, new learning rate = 0.009025, number of evaluations 12\n",
      "epoch 3: valid acc = 0.204, new learning rate = 0.00857375, number of evaluations 12\n",
      "epoch 4: valid acc = 0.276, new learning rate = 0.0081450625, number of evaluations 12\n",
      "epoch 5: valid acc = 0.32, new learning rate = 0.007737809374999999, number of evaluations 12\n",
      "epoch 6: valid acc = 0.422, new learning rate = 0.007350918906249998, number of evaluations 12\n",
      "epoch 7: valid acc = 0.39, new learning rate = 0.006983372960937498, number of evaluations 12\n",
      "epoch 8: valid acc = 0.482, new learning rate = 0.006634204312890623, number of evaluations 12\n",
      "epoch 9: valid acc = 0.626, new learning rate = 0.006302494097246091, number of evaluations 12\n",
      "epoch 10: valid acc = 0.6, new learning rate = 0.005987369392383786, number of evaluations 12\n",
      "epoch 11: valid acc = 0.706, new learning rate = 0.005688000922764597, number of evaluations 12\n",
      "epoch 12: valid acc = 0.668, new learning rate = 0.005403600876626367, number of evaluations 12\n",
      "epoch 13: valid acc = 0.66, new learning rate = 0.005133420832795048, number of evaluations 12\n",
      "epoch 14: valid acc = 0.724, new learning rate = 0.0048767497911552955, number of evaluations 12\n",
      "epoch 15: valid acc = 0.696, new learning rate = 0.00463291230159753, number of evaluations 12\n"
     ]
    }
   ],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import SGDNestMomentumOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = SGDNestMomentumOptim(model, momentum=0.9)\n",
    "hist_sgd_nesterov = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=15, batch_size=200, learning_rate=1e-2, \n",
    "    learning_decay=0.95, verbose=False, record_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5ZEf-udfmnv"
   },
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the algorithm above, we're already implying the notion of **[moment](https://en.wikipedia.org/wiki/Moment_(mathematics)) estimation** (i.e. $\\hat v_{t+1}$) in the sense of Nesterov by combining current gradient and exponetially decaying past gradients (with factor $\\beta$). Here we explore another more flexible and widely used estimation method - the **moving average**.\n",
    "\n",
    "For any time series denoted by $x_0, x_1, \\dots, x_t$, its moving average at time $t$ is defined iteratively as a weighted summation of the previous average and current value, i.e.\n",
    "\n",
    "$$\\mu_t = \\beta \\mu_{t-1} + (1 - \\beta) x_t$$\n",
    "\n",
    "where the hyperparameter $\\beta \\in [0, 1]$ is the decay factor controlling the influence of the past to the present. This provides the algorithm with better adaptations to the change of parameters through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, we would want to know *statistically* how far our moving average $\\mu_t$ deviates from the true average $\\mu$ by considering its expectation $\\mathbb{E}[\\mu_t]$.\n",
    "\n",
    "Specifically, the moving average can be expanded (by plugging in each $\\mu_t$) to\n",
    "\n",
    "$$\\mu_t = (1 - \\beta) \\sum_{i=1}^t \\beta^{t-i} x_i$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mu_t]\n",
    "= \\mathbb{E} \\left[ (1 - \\beta) \\sum_{i=1}^t \\beta^{t-i} x_i \\right]\n",
    "= (1 - \\beta) \\sum_{i=1}^t \\beta^{t-i} \\mathbb{E}[x_i]\n",
    "$$\n",
    "\n",
    "Assume that the series elements are identically distributed (i.e. $\\mathbb{E}[x_1] = \\dots = \\mathbb{E}[x_t] = \\mu$), then\n",
    "\n",
    "$$\\mathbb{E}[\\mu_t] = \\mu (1 - \\beta) \\sum_{i=1}^t \\beta^{t-i} = (1 - \\beta^t) \\mu$$\n",
    "\n",
    "Therefore, a bias-corrected moving average can be given by\n",
    "\n",
    "$$\\hat \\mu_t = \\frac{\\mu_t}{1 - \\beta^t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the discussions above, Adam (Adaptive Moment Estimation) algorithm can be formulated into three following steps.\n",
    "\n",
    "**1. Moment calculation**\n",
    "   - The 1st moment (**mean**) of the gradients:\n",
    "     $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "   - The 2nd moment (**variance**) of the gradients:\n",
    "     $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Bias correction**\n",
    "\n",
    "   - Adjust the 1st moment:\n",
    "   $$\\hat m_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "   - Adjust the 2nd moment:\n",
    "   $$\\hat v_t = \\frac{v_t}{1 - \\beta_2^t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Parameter update**\n",
    "\n",
    "   $$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t}+\\epsilon} \\odot \\hat m_t$$\n",
    "\n",
    "   Here, $\\epsilon$ is a small value (e.g. 1e-8) serves to avoid zero-division and $\\odot$ denotes the Hadamard (element-wise) product.\n",
    "\n",
    "The final accuracy you should expect is arround 0.7-0.9. \n",
    "\n",
    "**This is usually (not always) the optimal choice in practice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Implement Adam by editing `AdamOptim` in **./utils/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5220,
     "status": "ok",
     "timestamp": 1631139271319,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "aTws4phjfmnw",
    "outputId": "90f0732d-6062-4b8e-ffaf-6978d05958bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 10\n",
      "epoch 1: valid acc = 0.218, new learning rate = 0.00095, number of evaluations 12\n",
      "epoch 2: valid acc = 0.224, new learning rate = 0.0009025, number of evaluations 12\n",
      "epoch 3: valid acc = 0.202, new learning rate = 0.000857375, number of evaluations 12\n",
      "epoch 4: valid acc = 0.332, new learning rate = 0.0008145062499999999, number of evaluations 12\n",
      "epoch 5: valid acc = 0.39, new learning rate = 0.0007737809374999998, number of evaluations 12\n",
      "epoch 6: valid acc = 0.48, new learning rate = 0.0007350918906249997, number of evaluations 12\n",
      "epoch 7: valid acc = 0.484, new learning rate = 0.0006983372960937497, number of evaluations 12\n",
      "epoch 8: valid acc = 0.59, new learning rate = 0.0006634204312890621, number of evaluations 12\n",
      "epoch 9: valid acc = 0.634, new learning rate = 0.000630249409724609, number of evaluations 12\n",
      "epoch 10: valid acc = 0.652, new learning rate = 0.0005987369392383785, number of evaluations 12\n",
      "epoch 11: valid acc = 0.722, new learning rate = 0.0005688000922764595, number of evaluations 12\n",
      "epoch 12: valid acc = 0.764, new learning rate = 0.0005403600876626365, number of evaluations 12\n",
      "epoch 13: valid acc = 0.766, new learning rate = 0.0005133420832795047, number of evaluations 12\n",
      "epoch 14: valid acc = 0.754, new learning rate = 0.00048767497911552944, number of evaluations 12\n",
      "epoch 15: valid acc = 0.742, new learning rate = 0.00046329123015975297, number of evaluations 12\n"
     ]
    }
   ],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import AdamOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = AdamOptim(model, beta1=0.9, beta2=0.99)\n",
    "hist_adam = optimizer.train(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    num_epoch=15, batch_size=200, learning_rate=1e-3, \n",
    "    learning_decay=0.95, verbose=False, record_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdamW  (Adam with Weight Decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdamW** is a variant of the **Adam** optimizer that decouples the **weight decay** term from the gradient-based updates, making it more effective for improving generalization. Unlike the traditional Adam optimizer, which adds the weight decay term to the gradient update, AdamW explicitly separates these two aspects.\n",
    "\n",
    "The update rule for AdamW is formulated as follows:\n",
    "\n",
    "1. **Compute biased first moment (mean of gradients)**:\n",
    "   $$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "   $$\n",
    "   where $m_t$ represents the moving average of the gradient, $\\beta_1$ is the decay rate for the first moment, and $g_t$ is the gradient at time step $t$.\n",
    "\n",
    "2. **Compute biased second moment (uncentered variance of gradients)**:\n",
    "   $$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "   $$\n",
    "   where $v_t$ represents the moving average of the squared gradient, and $\\beta_2$ is the decay rate for the second moment.\n",
    "\n",
    "3. **Bias correction for moments**:\n",
    "   - **First moment (bias-corrected)**:\n",
    "   $$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "   $$\n",
    "   - **Second moment (bias-corrected)**:\n",
    "   $$\n",
    "   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $$\n",
    "\n",
    "4. **Parameter update with weight decay**:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t \\right)\n",
    "   $$\n",
    "   where:\n",
    "   - $\\eta$ is the **learning rate**.\n",
    "   - $\\epsilon$ is a small constant (e.g., $10^{-8}$) to prevent division by zero.\n",
    "   - $\\lambda$ is the **weight decay** coefficient.\n",
    "\n",
    "The final accuracy you should expect is arround **0.7-0.9**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Implement AdamW by editing `AdamWOptim` in **./utils/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification code for your implemention\n",
    "# Please don't change anything.\n",
    "\n",
    "from utils.optimizers import AdamWOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = AdamWOptim(model, beta1=0.9, beta2=0.999, eps=1e-8, l2_coeff = 1)\n",
    "hist_adamw = optimizer.train(X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=15, batch_size=200, learning_rate=1e-3, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop (Root Mean Square Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSprop** is an adaptive learning rate optimization algorithm designed to deal with the challenges of adjusting learning rates, especially in scenarios where gradients can vary widely. RMSprop aims to resolve issues like oscillations and slow convergence by normalizing the learning rate based on recent gradients.\n",
    "\n",
    "The update rule for **RMSprop** is formulated as follows:\n",
    "\n",
    "1. **Compute the moving average of squared gradients**:\n",
    "   $$\n",
    "   v_t = \\beta v_{t-1} + (1 - \\beta) g_t^2\n",
    "   $$\n",
    "   where $v_t$ represents the moving average of the squared gradients, $\\beta$ is a hyperparameter that controls the decay rate, and $g_t$ is the gradient at time step $t$.\n",
    "\n",
    "2. **Update the parameters**:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} g_t\n",
    "   $$\n",
    "   where $\\eta$ is the **learning rate**, and $\\epsilon$ is a small value (e.g., $10^{-8}$) added to prevent division by zero.\n",
    "\n",
    "The final accuracy you should expect is arround **0.7-0.9**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Implement RMSProp by editing `RMSpropOptim` in **./utils/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimizers import RMSpropOptim\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1], hidden_dims=[100, 100],\n",
    "    num_classes=10, weight_scale=1e-3, l2_reg=0.0\n",
    ")\n",
    "optimizer = RMSpropOptim(model, gamma=0.9, eps=1e-12)\n",
    "hist_rms = optimizer.train(X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=15, batch_size=200, learning_rate=1e-3, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dga2dgGxfmny"
   },
   "source": [
    "## Part 2: Comparison (4%)\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Run the following cells, which plot the loss, training accuracy, and validation accuracy curves of different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mzui75QUfmny"
   },
   "outputs": [],
   "source": [
    "loss_hist_sgd, train_acc_hist_sgd, val_acc_hist_sgd = hist_sgd\n",
    "loss_hist_momentum, train_acc_hist_momentum, val_acc_hist_momentum = hist_sgd_momentum\n",
    "loss_hist_nesterov, train_acc_hist_nesterov, val_acc_hist_nesterov = hist_sgd_nesterov\n",
    "loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = hist_adam\n",
    "loss_hist_adamw, train_acc_hist_adamw, val_acc_hist_adamw = hist_adamw\n",
    "loss_hist_rms, train_acc_hist_rms, val_acc_hist_rms = hist_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1631139318716,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "oX9CyZjLfmnz",
    "outputId": "ce4a99cd-8663-4a2c-cdca-6ab7b95125c9"
   },
   "outputs": [],
   "source": [
    "# Plot the training error curves for implemented optimizers\n",
    "plt.plot(loss_hist_sgd, label=\"sgd\")\n",
    "plt.plot(loss_hist_momentum, label=\"momentum\")\n",
    "plt.plot(loss_hist_nesterov, label=\"nesterov\")\n",
    "plt.plot(loss_hist_adam, label=\"adam\")\n",
    "plt.plot(loss_hist_adamw, label=\"adamw\")\n",
    "plt.plot(loss_hist_rms, label=\"rms\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1631139328348,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "JVigWVU1fmn0",
    "outputId": "c836fcfb-5350-4fbc-ccee-32270aa834e8"
   },
   "outputs": [],
   "source": [
    "# Plot the training accuracy curves for implemented optimizers\n",
    "plt.plot(train_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(train_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(train_acc_hist_nesterov, label=\"nesterov\")\n",
    "plt.plot(train_acc_hist_adam, label=\"adam\")\n",
    "plt.plot(train_acc_hist_adamw, label=\"adamw\")\n",
    "plt.plot(train_acc_hist_rms, label=\"rms\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1631139339586,
     "user": {
      "displayName": "Sung Jun Won",
      "photoUrl": "",
      "userId": "15792990474350106348"
     },
     "user_tz": 240
    },
    "id": "7AIf6zjpfmn1",
    "outputId": "25c10de5-13a4-40f9-e721-68f2d9ceb852"
   },
   "outputs": [],
   "source": [
    "# Plot the validation accuracy curves for implemented optimizers\n",
    "plt.plot(val_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(val_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(val_acc_hist_nesterov, label=\"nesterov\")\n",
    "plt.plot(val_acc_hist_adam, label=\"adam\")\n",
    "plt.plot(val_acc_hist_adamw, label=\"adamw\")\n",
    "plt.plot(val_acc_hist_rms, label=\"rms\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4aY4q4wfmn2"
   },
   "source": [
    "<span style=\"color:red\">**TODO:**</span> Compare the results from above. Answer the following questions based on your observations and understandings of these optimizers:\n",
    "\n",
    "1. Briefly conclude why each of the optimizers beats naive SGD. Which of them is the winner based on your results?\n",
    "2. Discuss the trade-offs between using a simple optimizer like SGD versus a more complex one like Adam. In what scenarios might you prefer one over the other?\n",
    "3. Briefly describe another optimizer that is not introduced in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL5H31uzfmn3"
   },
   "source": [
    "Answer: **[fill in here]**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
